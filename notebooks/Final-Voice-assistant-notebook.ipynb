{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e6c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain_core langchain_community langchain_groq SpeechRecognition pyttsx3 pyaudio yt-dlp python-vlc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be15567",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e54fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simmu\\AppData\\Local\\Temp\\ipykernel_28956\\3129996719.py:50: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
      "C:\\Users\\simmu\\AppData\\Local\\Temp\\ipykernel_28956\\3129996719.py:53: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  qa_chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize Groq LLM\n",
    "groq_api_key = \"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\"  # Replace with your actual API key\n",
    "llm = ChatGroq(\n",
    "    api_key=groq_api_key,\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Custom Prompt\n",
    "custom_prompt_template_for_chatbot = \"\"\"\n",
    "You are a knowledgeable assistant specializing in Data Science and Artificial Intelligence (AI).\n",
    "\n",
    "Your primary objective is to assist students by providing clear, concise, and accurate answers to their questions specifically related to Data Science and AI. This includes, but is not limited to, the following topics:\n",
    "- Programming languages and tools: Python, SQL (MySQL, SQLite, MongoDB)\n",
    "- Data visualization tools: Power BI, Tableau\n",
    "- Statistical concepts and methodologies\n",
    "- Machine Learning (ML) techniques and frameworks\n",
    "- MLFlow for managing machine learning workflows\n",
    "- Containerization with Docker\n",
    "- Deep Learning concepts and frameworks\n",
    "- Natural Language Processing (NLP)\n",
    "- Generative AI technologies\n",
    "- Skills required for a career in Data Science and AI\n",
    "\n",
    "When responding, ensure that your answers are focused and straightforward, avoiding unnecessary details. If users ask complex questions, break down your responses into manageable parts and provide step-by-step explanations when needed.\n",
    "\n",
    "Always be polite and encouraging, ensuring that you provide accurate information at all times.\n",
    "\n",
    "Remember previous exchanges in the conversation to provide better context for your responses.\n",
    "\n",
    "If a question is asked that falls outside the realm of Data Science and AI or does not relate to the topics mentioned above, respond with a polite message indicating that the question is unrelated. For example: \"I'm sorry, but that topic is outside the scope of Data Science and AI. I'm unable to provide an answer.\"\n",
    "\n",
    "Question: {history} Current question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=custom_prompt_template_for_chatbot,\n",
    "    input_variables=[\"history\", \"question\"],\n",
    ")\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
    "\n",
    "# Create chain with memory\n",
    "qa_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "\n",
    "# Function to Handle Q&A\n",
    "def handle_qna(user_input):\n",
    "    \"\"\"\n",
    "    Handles user queries for Q&A functionality.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = qa_chain.run(user_input)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while processing your question: {e}\"\n",
    "\n",
    "# Integration into Chat Assistant\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Q&A Module Test: Ask me anything related to Data Science and AI!\")\n",
    "#     while True:\n",
    "#         user_question = input(\"You: \")\n",
    "#         if user_question.lower() in [\"exit\", \"quit\"]:\n",
    "#             print(\"Goodbye!\")\n",
    "#             break\n",
    "#         print(f\"Assistant: {handle_qna(user_question)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc261f42",
   "metadata": {},
   "source": [
    "## Set remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272638ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import threading\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Global dictionary to store active reminders\n",
    "reminders = []\n",
    "conversation_context = {}\n",
    "\n",
    "def normalize_time(user_input):\n",
    "    \"\"\"\n",
    "    Normalize unconventional time inputs into a valid 12-hour HH:MM AM/PM format.\n",
    "    For example:\n",
    "    - '914pm' -> '09:14 PM'\n",
    "    - '915pm' -> '09:15 PM'\n",
    "    - '9pm' -> '09:00 PM'\n",
    "    \"\"\"\n",
    "    # Extract numbers and AM/PM\n",
    "    match = re.match(r'(\\d{1,4})(AM|PM|am|pm)?', user_input, re.IGNORECASE)\n",
    "    if not match:\n",
    "        return None  # Invalid input\n",
    "    \n",
    "    raw_time = match.group(1)  # Extract the numeric part\n",
    "    period = match.group(2).upper() if match.group(2) else \"PM\"  # Default to PM if not provided\n",
    "\n",
    "    # Handle different lengths of the raw_time\n",
    "    if len(raw_time) == 1 or len(raw_time) == 2:  # e.g., '9' or '12'\n",
    "        hour = int(raw_time)\n",
    "        minute = 0\n",
    "    elif len(raw_time) == 3:  # e.g., '914' -> hour: 9, minute: 14\n",
    "        hour = int(raw_time[0])\n",
    "        minute = int(raw_time[1:])\n",
    "    elif len(raw_time) == 4:  # e.g., '0915' or '915' -> hour: 9, minute: 15\n",
    "        hour = int(raw_time[:2])\n",
    "        minute = int(raw_time[2:])\n",
    "    else:\n",
    "        return None  # Invalid input length\n",
    "\n",
    "    # Adjust hour and minute for valid time ranges\n",
    "    while minute >= 60:\n",
    "        hour += 1\n",
    "        minute -= 60\n",
    "\n",
    "    # Convert to 12-hour format\n",
    "    if hour > 12:\n",
    "        hour %= 12\n",
    "    if hour == 0:\n",
    "        hour = 12\n",
    "\n",
    "    # Format into HH:MM AM/PM\n",
    "    try:\n",
    "        normalized_time = datetime.strptime(f\"{hour}:{minute:02d} {period}\", \"%I:%M %p\")\n",
    "        return normalized_time.strftime(\"%I:%M %p\")\n",
    "    except ValueError:\n",
    "        return None  # Invalid input after adjustments\n",
    "\n",
    "# Reminder Handler\n",
    "def handle_reminder(user_input):\n",
    "    \"\"\"\n",
    "    Handles setting reminders interactively with the user.\n",
    "    \"\"\"\n",
    "    global conversation_context\n",
    "\n",
    "    # If there's a pending_action in conversation_context, continue that flow\n",
    "    if \"pending_action\" in conversation_context:\n",
    "        action = conversation_context.pop(\"pending_action\")\n",
    "        if action == \"set_reminder_time\":\n",
    "            # Normalize time input\n",
    "            normalized_time = normalize_time(user_input)\n",
    "            if not normalized_time:\n",
    "                return \"Invalid time format. Please provide the time in HH:MM AM/PM format (e.g., 02:30 PM).\"\n",
    "            conversation_context[\"reminder_time\"] = normalized_time\n",
    "            conversation_context[\"pending_action\"] = \"set_reminder_message\"\n",
    "            return \"What should I remind you about?\"\n",
    "        elif action == \"set_reminder_message\":\n",
    "            # Now we have time and message, we can set the reminder\n",
    "            reminder_time = conversation_context.pop(\"reminder_time\")\n",
    "            reminder_message = user_input\n",
    "            set_reminder(reminder_time, reminder_message)\n",
    "            return f\"Reminder set for {reminder_time} with message: '{reminder_message}'\"\n",
    "    else:\n",
    "        # Start the reminder setting process\n",
    "        conversation_context[\"pending_action\"] = \"set_reminder_time\"\n",
    "        return \"What time should I set the reminder for? (e.g., 02:30 PM)\"\n",
    "\n",
    "def set_reminder(reminder_time, message):\n",
    "    \"\"\"\n",
    "    Create a background thread that triggers at the exact reminder_time (12-hour format).\n",
    "    \"\"\"\n",
    "    def reminder_thread():\n",
    "        while True:\n",
    "            current_time = datetime.now().strftime(\"%I:%M %p\")  # 12-hour format\n",
    "            if current_time == reminder_time:\n",
    "                print(f\"\\nReminder: {message}\")\n",
    "                break\n",
    "            time.sleep(1)\n",
    "\n",
    "    reminders.append({\"time\": reminder_time, \"message\": message})\n",
    "    threading.Thread(target=reminder_thread, daemon=True).start()\n",
    "\n",
    "# Test Reminder Functionality\n",
    "# if __name__ == \"__main__\":\n",
    "#     conversation_context = {}  # Initialize global conversation context\n",
    "#     print(\"Welcome to the Reminder Module!\")\n",
    "#     print(\"You can interactively set reminders.\")\n",
    "\n",
    "#     while True:\n",
    "#         user_input = input(\"You: \").strip()\n",
    "#         if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "#             print(\"Goodbye!\")\n",
    "#             break\n",
    "#         response = handle_reminder(user_input)\n",
    "#         print(f\"Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc140e6",
   "metadata": {},
   "source": [
    "## Weather Fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f22cc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather(location):\n",
    "    \"\"\"\n",
    "    Fetch the weather details for a given location using Open-Meteo.\n",
    "    \"\"\"\n",
    "    geocode_url = f\"https://geocoding-api.open-meteo.com/v1/search?name={location}\"\n",
    "\n",
    "    try:\n",
    "        # Fetch latitude and longitude\n",
    "        geocode_response = requests.get(geocode_url)\n",
    "        geocode_response.raise_for_status()\n",
    "        geocode_data = geocode_response.json()\n",
    "\n",
    "        if \"results\" not in geocode_data or len(geocode_data[\"results\"]) == 0:\n",
    "            return f\"Sorry, I couldn't find weather details for '{location}'. Please try another location.\"\n",
    "\n",
    "        latitude = geocode_data[\"results\"][0][\"latitude\"]\n",
    "        longitude = geocode_data[\"results\"][0][\"longitude\"]\n",
    "        location_name = geocode_data[\"results\"][0][\"name\"]\n",
    "\n",
    "        # Fetch weather details\n",
    "        weather_url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "        weather_response = requests.get(weather_url)\n",
    "        weather_response.raise_for_status()\n",
    "        weather_data = weather_response.json()\n",
    "\n",
    "        if \"current_weather\" in weather_data:\n",
    "            current_weather = weather_data[\"current_weather\"]\n",
    "            temperature = current_weather[\"temperature\"]\n",
    "            windspeed = current_weather[\"windspeed\"]\n",
    "            weather_code = current_weather.get(\"weathercode\", -1)\n",
    "\n",
    "            weather_conditions = {\n",
    "                0: \"Clear sky\",\n",
    "                1: \"Mainly clear\",\n",
    "                2: \"Partly cloudy\",\n",
    "                3: \"Overcast\",\n",
    "                45: \"Foggy\",\n",
    "                48: \"Depositing rime fog\",\n",
    "                51: \"Light drizzle\",\n",
    "                53: \"Moderate drizzle\",\n",
    "                55: \"Dense drizzle\",\n",
    "                61: \"Slight rain\",\n",
    "                63: \"Moderate rain\",\n",
    "                65: \"Heavy rain\",\n",
    "                71: \"Slight snow\",\n",
    "                73: \"Moderate snow\",\n",
    "                75: \"Heavy snow\",\n",
    "                80: \"Rain showers\",\n",
    "                81: \"Moderate rain showers\",\n",
    "                82: \"Heavy rain showers\",\n",
    "                95: \"Thunderstorm\",\n",
    "                96: \"Thunderstorm with hail\",\n",
    "            }\n",
    "            weather_description = weather_conditions.get(weather_code, \"Unknown conditions\")\n",
    "\n",
    "            temperature_description = (\n",
    "                \"hot\" if temperature > 30 else \"cold\" if temperature < 15 else \"moderate\"\n",
    "            )\n",
    "\n",
    "            return (f\"The current weather in {location_name} is {weather_description} with a temperature of \"\n",
    "                    f\"{temperature}°C ({temperature_description}) and a windspeed of {windspeed} km/h.\")\n",
    "        else:\n",
    "            return \"Sorry, I couldn't fetch the weather details at this time.\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"An error occurred while fetching weather data: {e}\"\n",
    "\n",
    "def handle_weather(user_input):\n",
    "    \"\"\"\n",
    "    Handles weather-related queries interactively with the user.\n",
    "    \"\"\"\n",
    "    global current_functionality, conversation_context\n",
    "\n",
    "    # Directly fetch weather if functionality is already set to 'weather'\n",
    "    if current_functionality == \"weather\":\n",
    "        if user_input.lower() == \"exit\":\n",
    "            current_functionality = None\n",
    "            return \"Exited the weather functionality. How can I assist you next?\"\n",
    "        return fetch_weather(user_input.strip())\n",
    "\n",
    "    # Detect if location is already mentioned in the first input\n",
    "    if user_input.lower() in [\"weather\", \"weather update\", \"what's the weather outside\"]:\n",
    "        current_functionality = \"weather\"\n",
    "        return \"For which location would you like to get the weather?\"\n",
    "\n",
    "    # Assume the user input is a location on the first call\n",
    "    current_functionality = \"weather\"\n",
    "    return fetch_weather(user_input.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039eef35",
   "metadata": {},
   "source": [
    "## Play Music from youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad15af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agent for music\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "music_player = None  # Global music player\n",
    "\n",
    "# Initialize LLM for Music Query Processing\n",
    "llm_music = ChatGroq(\n",
    "    api_key=\"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\",\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Few-shot examples for music queries\n",
    "music_few_shot_examples = [\n",
    "    {\"input\": \"Play the latest song by Taylor Swift.\", \"query\": \"latest song by Taylor Swift\"},\n",
    "    {\"input\": \"I want to listen to some relaxing piano music.\", \"query\": \"relaxing piano music\"},\n",
    "    {\"input\": \"Play Pushpa 2 title song.\", \"query\": \"Pushpa 2 title song\"},\n",
    "    {\"input\": \"Find and play some jazz music.\", \"query\": \"jazz music\"},\n",
    "    {\"input\": \"Despacito song\", \"query\": \"Despacito song\"},\n",
    "    {\"input\": \"I want the title song from the movie Pushpa.\", \"query\": \"title song from the movie Pushpa\"},\n",
    "    {\"input\": \"Play the Devara Telugu movie title song.\", \"query\": \"Devara Telugu movie title song\"},\n",
    "    {\"input\": \"Play something classical.\", \"query\": \"classical music\"},\n",
    "    {\"input\": \"Can you find a remix of Shape of You?\", \"query\": \"Shape of You remix\"},\n",
    "    {\"input\": \"Play Arijit Singh's latest hit.\", \"query\": \"Arijit Singh latest hit song\"},\n",
    "    {\"input\": \"Find and play a calming meditation track.\", \"query\": \"calming meditation track\"},\n",
    "    {\"input\": \"I want to hear Bollywood romantic songs.\", \"query\": \"Bollywood romantic songs\"},\n",
    "    {\"input\": \"Play a workout playlist.\", \"query\": \"workout playlist\"},\n",
    "    {\"input\": \"Do you have any rock music?\", \"query\": \"rock music\"},\n",
    "    {\"input\": \"Find a Telugu devotional song for me.\", \"query\": \"Telugu devotional song\"},\n",
    "    {\"input\": \"Play Ed Sheeran's Perfect.\", \"query\": \"Ed Sheeran Perfect\"},\n",
    "    {\"input\": \"Play the background score of Interstellar.\", \"query\": \"Interstellar background score\"},\n",
    "    {\"input\": \"Can you play 'Kala Chashma'?\", \"query\": \"Kala Chashma song\"},\n",
    "    {\"input\": \"I want to listen to a live version of Rolling in the Deep.\", \"query\": \"live version of Rolling in the Deep\"},\n",
    "    {\"input\": \"Find a trending pop song.\", \"query\": \"trending pop song\"},\n",
    "    {\"input\": \"Play some 90s hits.\", \"query\": \"90s hits\"},\n",
    "    {\"input\": \"Find an acoustic version of Hotel California.\", \"query\": \"acoustic version of Hotel California\"},\n",
    "    {\"input\": \"Play a party anthem.\", \"query\": \"party anthem\"},\n",
    "    {\"input\": \"Play something by Imagine Dragons.\", \"query\": \"Imagine Dragons songs\"},\n",
    "    {\"input\": \"I want to listen to Lofi beats.\", \"query\": \"Lofi beats\"},\n",
    "    {\"input\": \"Can you play the theme song from Harry Potter?\", \"query\": \"Harry Potter theme song\"},\n",
    "    {\"input\": \"Play a motivational song.\", \"query\": \"motivational song\"},\n",
    "    {\"input\": \"Find a Tamil melody.\", \"query\": \"Tamil melody\"},\n",
    "    {\"input\": \"Play the OST from Game of Thrones.\", \"query\": \"Game of Thrones OST\"},\n",
    "    {\"input\": \"Can you play a recent K-pop hit?\", \"query\": \"recent K-pop hit\"},\n",
    "    {\"input\": \"Play the soundtrack of Titanic.\", \"query\": \"Titanic soundtrack\"}\n",
    "]\n",
    "\n",
    "\n",
    "def refine_music_query(user_input):\n",
    "    \"\"\"\n",
    "    Use LLM to extract or infer the music query from user input with enhanced accuracy.\n",
    "    \"\"\"\n",
    "    few_shot_text = \"\\n\".join([f\"Input: {ex['input']} Query: {ex['query']}\" for ex in music_few_shot_examples])\n",
    "    prompt = f\"\"\"\n",
    "        You are a music query extraction assistant. Your task is to strictly extract the specific song name and language (if mentioned) \n",
    "        from the user's input. Follow these rules:\n",
    "\n",
    "        1. Output only the song name or genre and the language (if specified).\n",
    "        2. Do not include any additional interpretation, explanation, or context.\n",
    "        3. If the input is unclear or ambiguous, return only the most relevant key terms directly related to the song or genre or print the user input directly if it is not clear.\n",
    "\n",
    "        Here are examples:\n",
    "\n",
    "        {few_shot_text}\n",
    "\n",
    "        Input: {user_input}\n",
    "        Query:\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a music query refinement assistant, specializing in accurate song identification.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    response = llm_music.invoke(messages)\n",
    "    refined_query = response.content.strip()\n",
    "    print(f\"[DEBUG] Refined Query: {refined_query}\")\n",
    "    return refined_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5d47378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import vlc\n",
    "\n",
    "# Global music player\n",
    "music_player = None\n",
    "\n",
    "def fetch_and_play_music(query):\n",
    "    \"\"\"\n",
    "    Searches for music on YouTube and streams it directly using VLC.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure yt-dlp to extract the streaming URL\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio/best',\n",
    "            'noplaylist': True,\n",
    "            'quiet': True,\n",
    "        }\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            results = ydl.extract_info(f\"ytsearch:{query}\", download=False)\n",
    "            if not results.get('entries'):\n",
    "                return \"No results found for your query. Please try a different song.\"\n",
    "\n",
    "            # Extract the first result\n",
    "            result = results['entries'][0]\n",
    "            video_title = result['title']\n",
    "            video_url = result['url']\n",
    "\n",
    "            # Stop any currently playing music\n",
    "            stop_music()\n",
    "\n",
    "            # Add headers for VLC to handle YouTube URLs\n",
    "            media = vlc.Media(video_url)\n",
    "            media.add_options(\n",
    "                \":http-user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "                \":http-referrer=https://www.youtube.com/\",\n",
    "                \":network-caching=1000\"\n",
    "            )\n",
    "\n",
    "            # Play the stream\n",
    "            global music_player\n",
    "            music_player = vlc.MediaPlayer()\n",
    "            music_player.set_media(media)\n",
    "            music_player.audio_set_volume(100)  # Set volume to 100%\n",
    "            music_player.play()\n",
    "\n",
    "            return f\"Playing: {video_title}. Say 'Exit' or 'Quit to stop the music.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while playing the song: {e}\"\n",
    "\n",
    "\n",
    "def stop_music():\n",
    "    \"\"\"\n",
    "    Stops the currently playing music, if any.\n",
    "    \"\"\"\n",
    "    global music_player\n",
    "    if music_player:\n",
    "        music_player.stop()\n",
    "        music_player = None\n",
    "\n",
    "\n",
    "def handle_play_music(user_input):\n",
    "    \"\"\"\n",
    "    Handles user requests for music playback.\n",
    "    \"\"\"\n",
    "    # Handle stop-related commands\n",
    "    if user_input.lower().strip() in [\"stop\", \"exit\", \"quit\"]:\n",
    "        stop_music()  # Stop the music playback\n",
    "\n",
    "    # Refine the query for playing music (if applicable)\n",
    "    refined_query = user_input.strip()  # For notebook, use raw input directly\n",
    "    if not refined_query:\n",
    "        return \"I couldn't understand the song you want to play. Could you try rephrasing?\"\n",
    "\n",
    "    # Stop any currently playing music before starting a new one\n",
    "    stop_music()\n",
    "\n",
    "    # Fetch and play the song\n",
    "    return fetch_and_play_music(refined_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f981c",
   "metadata": {},
   "source": [
    "## Main Code - Voice Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "166f7182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello! Welcome to your Voice Assistant.\n",
      "Assistant: You can ask me to set reminders, fetch weather, play music, or ask general questions.\n",
      "Assistant: Say 'quit' at any time to end our session.\n",
      "Assistant: Listening...\n",
      "Assistant: Sorry, I didn't catch that. Please try again.\n",
      "Assistant: Listening...\n",
      "You: what is the capital of France\n",
      "[DEBUG] Detected Intent: qna\n",
      "[DEBUG] Detected Intent: qna\n",
      "[DEBUG] Continuing in qna functionality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simmu\\AppData\\Local\\Temp\\ipykernel_28956\\3129996719.py:66: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(user_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I'm sorry, but that topic is outside the scope of Data Science and AI. I'm unable to provide an answer. However, I would be happy to help you with any questions related to Data Science and Artificial Intelligence!\n",
      "Assistant: Listening...\n",
      "You: can you explain deep learning\n",
      "[DEBUG] Continuing in qna functionality.\n",
      "Assistant: Absolutely, I'd be happy to explain deep learning!\n",
      "\n",
      "Deep learning is a subset of machine learning, which in turn is a subset of artificial intelligence. It's a method of data analysis that automates the building of analytical models. It's called \"deep\" learning because it involves multiple layers in the neural network, which is a key component of deep learning models.\n",
      "\n",
      "Deep learning models are designed to automatically and adaptively learn to represent data by training on a large amount of data with multiple layers of non-linear processing. This allows the model to learn complex patterns and representations from the data.\n",
      "\n",
      "Deep learning has been successful in various applications such as image and speech recognition, natural language processing, and game playing. It's made possible by the availability of large datasets and powerful computational resources, such as graphics processing units (GPUs) and tensor processing units (TPUs).\n",
      "\n",
      "Some popular deep learning frameworks include TensorFlow, PyTorch, and Keras. These frameworks provide tools and libraries for building and training deep learning models.\n",
      "\n",
      "I hope this explanation helps! Let me know if you have any more questions about deep learning or any other topic related to Data Science and AI.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 197\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m########################################\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# 8) Entry point\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m########################################\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# Just run voice assistant (no text-based chat)\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m     \u001b[43mvoice_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 189\u001b[0m, in \u001b[0;36mvoice_assistant\u001b[1;34m()\u001b[0m\n\u001b[0;32m    187\u001b[0m response \u001b[38;5;241m=\u001b[39m process_user_input(user_input)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Speak the result\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m \u001b[43mspeak\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m, in \u001b[0;36mspeak\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m tts_engine\u001b[38;5;241m.\u001b[39msay(text)\n\u001b[1;32m---> 37\u001b[0m \u001b[43mtts_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunAndWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\Voice-Assistant\\Voice_Assistant\\voice\\lib\\site-packages\\pyttsx3\\engine.py:183\u001b[0m, in \u001b[0;36mEngine.runAndWait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inLoop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driverLoop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunAndWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\Voice-Assistant\\Voice_Assistant\\voice\\lib\\site-packages\\pyttsx3\\driver.py:195\u001b[0m, in \u001b[0;36mDriverProxy.runAndWait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mCalled by the engine to start an event loop, process all commands in\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03mthe queue at the start of the loop, and then exit the loop.\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_push(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mendLoop, \u001b[38;5;28mtuple\u001b[39m())\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartLoop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\Voice-Assistant\\Voice_Assistant\\voice\\lib\\site-packages\\pyttsx3\\drivers\\sapi5.py:146\u001b[0m, in \u001b[0;36mSAPI5Driver.startLoop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    145\u001b[0m pythoncom\u001b[38;5;241m.\u001b[39mPumpWaitingMessages()\n\u001b[1;32m--> 146\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import requests\n",
    "import yt_dlp\n",
    "import vlc\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "########################################\n",
    "# 2) Your unchanged global states & variables\n",
    "########################################\n",
    "current_functionality = None\n",
    "conversation_context = {}\n",
    "music_player = None\n",
    "\n",
    "########################################\n",
    "# 3) TTS (Text-to-Speech) Setup\n",
    "########################################\n",
    "tts_engine = pyttsx3.init()\n",
    "# Adjust speaking rate (200 is fairly normal speed)\n",
    "tts_engine.setProperty('rate', 200)\n",
    "\n",
    "def speak(text: str):\n",
    "    \"\"\"\n",
    "    Convert text to speech and also show it (like print).\n",
    "    \"\"\"\n",
    "    print(f\"Assistant: {text}\")\n",
    "    tts_engine.say(text)\n",
    "    tts_engine.runAndWait()\n",
    "\n",
    "########################################\n",
    "# 4) STT (Speech-to-Text) Setup\n",
    "########################################\n",
    "recognizer = sr.Recognizer()\n",
    "microphone = sr.Microphone()\n",
    "\n",
    "def listen_for_command() -> str:\n",
    "    \"\"\"\n",
    "    Listen via microphone and return recognized text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with microphone as source:\n",
    "            # Optional: adjust for ambient noise\n",
    "            recognizer.adjust_for_ambient_noise(source, duration=0.5)\n",
    "            speak(\"Listening...\")\n",
    "            audio = recognizer.listen(source)\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(f\"You: {text}\")\n",
    "        return text.strip()\n",
    "    except sr.UnknownValueError:\n",
    "        return \"\"\n",
    "    except sr.RequestError:\n",
    "        return \"\"\n",
    "\n",
    "########################################\n",
    "# 5) Your unchanged code from the question\n",
    "#    (We just copy/paste your logic exactly,\n",
    "#    so that we do not alter it.)\n",
    "########################################\n",
    "\n",
    "# -- LLM for intent detection (unchanged) --\n",
    "llm_intent = ChatGroq(\n",
    "    api_key=\"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\",\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "few_shot_examples = [\n",
    "    {\"input\": \"Can you explain what AI is?\", \"intent\": \"qna\"},\n",
    "    {\"input\": \"Remind me to call John at 3 PM.\", \"intent\": \"reminder\"},\n",
    "    {\"input\": \"What's the weather in Paris?\", \"intent\": \"weather\"},\n",
    "    {\"input\": \"Play a song from YouTube\", \"intent\": \"music\"},\n",
    "]\n",
    "\n",
    "def detect_intent(user_input):\n",
    "    \"\"\"\n",
    "    Use LLM to detect the intent of the user's query.\n",
    "    \"\"\"\n",
    "    few_shot_text = \"\\n\".join([f\"Input: {ex['input']} Intent: {ex['intent']}\" for ex in few_shot_examples])\n",
    "    prompt = f\"\"\"\n",
    "    Classify the user's query into one of these intents:\n",
    "    - qna\n",
    "    - reminder\n",
    "    - weather\n",
    "    - music\n",
    "\n",
    "    Examples:\n",
    "    {few_shot_text}\n",
    "\n",
    "    Input: {user_input}\n",
    "    Intent:\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are an intent detection assistant.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    response = llm_intent.invoke(messages)\n",
    "    intent_detected = response.content.strip().lower()\n",
    "    print(f\"[DEBUG] Detected Intent: {intent_detected}\")\n",
    "    return intent_detected\n",
    "\n",
    "# -------------\n",
    "# Next, your process_user_input(...) from the question,\n",
    "# exactly as written, with your same logic\n",
    "# (i.e. skipping user_input if inside a functionality, etc.)\n",
    "# -------------\n",
    "def process_user_input(user_input):\n",
    "    \"\"\"\n",
    "    Route the user input to the appropriate functionality \n",
    "    based on detected intent (unchanged from your question).\n",
    "    \"\"\"\n",
    "    global current_functionality, conversation_context, music_player\n",
    "\n",
    "    # If inside a functionality, bypass intent detection\n",
    "    if current_functionality:\n",
    "        print(f\"[DEBUG] Continuing in {current_functionality} functionality.\")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            if current_functionality == \"music\" and music_player:\n",
    "                music_player.stop()\n",
    "                music_player = None\n",
    "            current_functionality = None\n",
    "            conversation_context.clear()\n",
    "            return \"Exited the current functionality. How can I assist you next?\"\n",
    "\n",
    "        # If you have Q&A, reminder, weather, etc. placeholders:\n",
    "        if current_functionality == \"qna\":\n",
    "            return handle_qna(user_input)\n",
    "        elif current_functionality == \"reminder\":\n",
    "            return handle_reminder(user_input)\n",
    "        elif current_functionality == \"weather\":\n",
    "            return handle_weather(user_input)\n",
    "        elif current_functionality == \"music\":\n",
    "            return handle_play_music(user_input)\n",
    "\n",
    "\n",
    "    # If not in any functionality, detect the intent\n",
    "    intent = detect_intent(user_input)\n",
    "    print(f\"[DEBUG] Detected Intent: {intent}\")\n",
    "\n",
    "    # Switch functionalities or default to qna\n",
    "    if intent in [\"qna\", \"reminder\", \"weather\", \"music\", \"quiz\"]:\n",
    "        current_functionality = intent\n",
    "        return process_user_input(user_input)\n",
    "    elif intent == \"qna\" or intent not in [\"reminder\", \"weather\", \"music\"]:\n",
    "        current_functionality = \"qna\"\n",
    "        return handle_qna(user_input)\n",
    "    else:\n",
    "        return \"I'm sorry, I couldn't understand that. Could you try rephrasing your request?\"\n",
    "\n",
    "########################################\n",
    "# 7) The Voice Assistant Main Loop\n",
    "########################################\n",
    "def voice_assistant():\n",
    "    \"\"\"\n",
    "    Main voice-based loop that does:\n",
    "    - TTS for greeting\n",
    "    - STT to get user input\n",
    "    - Pass user input to your existing process_user_input\n",
    "    - TTS the response\n",
    "    - Repeat until user says \"quit\"\n",
    "    \"\"\"\n",
    "    speak(\"Hello! Welcome to your Voice Assistant.\")\n",
    "    speak(\"You can ask me to set reminders, fetch weather, play music, or ask general questions.\")\n",
    "    speak(\"Say 'quit' at any time to end our session.\")\n",
    "\n",
    "    while True:\n",
    "        # Listen for user speech\n",
    "        user_input = listen_for_command()\n",
    "        if not user_input:\n",
    "            speak(\"Sorry, I didn't catch that. Please try again.\")\n",
    "            continue\n",
    "\n",
    "        # If user says 'quit'\n",
    "        if user_input.lower() == \"quit\":\n",
    "            speak(\"Goodbye! Have a great day!\")\n",
    "            break\n",
    "\n",
    "        # Otherwise, process the input\n",
    "        response = process_user_input(user_input)\n",
    "        # Speak the result\n",
    "        speak(response)\n",
    "\n",
    "\n",
    "########################################\n",
    "# 8) Entry point\n",
    "########################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Just run voice assistant (no text-based chat)\n",
    "    voice_assistant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d1ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87674c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
